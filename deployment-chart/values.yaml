# Default values for deployment-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nameOverride: ""
fullnameOverride: ""

replicaCount: 1

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUAverageValue: 100m
  targetMemoryAverageValue: 200Mi

image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
  # following optional annotation indicates that we use GKE container-native load-balancing when exposing this service using a GKE ingress
  # annotations:
    # cloud.google.com/neg: '{"ingress": true}'


ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
      - path: /
        backend:
          serviceName: chart-example.local
          servicePort: 80
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  limits:
    cpu: 200m # arbitrarily high, adjust as per usage
    memory: 400Mi # arbitrarily high, adjust as per usage
  requests:
    cpu: 5m  # arbitrarily low, adjust as per usage
    memory: 20Mi  # arbitrarily low, adjust as per usage

nodeSelector: {}

tolerations: []

# ensures that pods from the same deployment don't end up on the same node
# `soft` antiAffinity provides this feature at pod schedule time only (best-effort)
# `hard` antiAffinity ensures that pods end up on different nodes
antiAffinity: "soft" # override to "hard" for critical apps

# use node affinity to ensure that pods end up or don't end up on specific nodes
# for example, for pods which require nodes with GPU available
nodeAffinity: {}
# Example nodeAffinity:
# nodeAffinity:
#   requiredDuringSchedulingIgnoredDuringExecution:
#     nodeSelectorTerms:
#     - matchExpressions:
#       - key: cloud.google.com/gke-accelerator
#         operator: Exists

probes:
  liveness:
    enabled: false
    path: /health
    port: http
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 3
  readiness:
    enabled: false
    path: /health
    port: http
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 3
  startup:
    enabled: false
    path: /health
    port: http
    initialDelaySeconds: 1
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6 # failureThreshold * periodSeconds = 60 seconds startup delay allowed by default

# sane pod rollout strategy, can usually leave this untouched
podRollout:
  minReadySeconds: 10
  maxSurge: 1
  maxUnavailable: 0
